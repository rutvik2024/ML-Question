# -*- coding: utf-8 -*-
"""Assignment1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Bvupxwpj4LGQWeZQe2OjVydLCF544YVA
"""

from google.colab import drive
drive.mount('/content/drive')

"""#install libraries"""

!pip install langchain-community
!pip install -q cassio datasets langchain openai tiktoken
## Embedding
!pip install install sentence_transformers
!pip install llama-cpp-python
!pip install langchain
!pip install ctransformers

"""#Load Json Data"""

import json
import re

json_path = "/content/drive/MyDrive/Dataset/news.article.json"

# Load the JSON file
with open(json_path, 'r') as file:
    data = json.load(file)

data[0]

len(data)

"""# filter data & Preprocess"""

# Extract articles
articles = [item.get('articleBody', '') for item in data]

len(articles)

# Function to clean text
def clean_text(text):
    text = re.sub(r'\s+', ' ', text)  # Remove extra whitespaces
    text = re.sub(r'[^\w\s]', '', text)  # Remove punctuation and special characters
    text = text.lower()  # Convert to lowercase
    return text

exm_txt = articles[0]
exm_txt

cln_exm_txt=clean_text(exm_txt)
cln_exm_txt

# Clean all articles
cleaned_articles = [clean_text(article) for article in articles]

len(cleaned_articles)

# Keywords related to Israel-Hamas war
keywords = ['israel', 'hamas', 'gaza', 'palestine', 'idf', 'al-shifa', 'war', 'conflict']

def is_relevant(article):
    return any(keyword in article for keyword in keywords)

# Filter relevant articles
relevant_articles = [article for article in cleaned_articles if is_relevant(article)]

len(relevant_articles)

"""#Text chunk and embedding"""

def extract_text_from_articles(relevant_articles):
    extracted_text = ""
    for article in relevant_articles:
        extracted_text += article + "\n"
    return extracted_text

extracted_text = extract_text_from_articles(relevant_articles[:1000])
print(extracted_text)

# Function to split text into chunks
def custom_text_splitter(text, chunk_size: int, chunk_overlap: int):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        if len(' '.join(current_chunk + [word])) <= chunk_size:
            current_chunk.append(word)
        else:
            chunks.append(' '.join(current_chunk))
            # Use overlap to start new chunk
            current_chunk = current_chunk[-chunk_overlap // len(current_chunk):] + [word]

    # Add the last chunk
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks

# Split extracted text into chunks
chunk_size = 800
chunk_overlap = 200
texts = custom_text_splitter(extracted_text, chunk_size, chunk_overlap)

texts[:5]

# Specify the file path for the new text file
file_path = "Data/text_file.txt"

# Write the content to the text file
with open(file_path, "w", encoding="utf-8") as file:
    file.write(extracted_text)

len(extracted_text)

"""#With vector database"""

from langchain.embeddings.huggingface import HuggingFaceEmbeddings
from huggingface_hub import hf_hub_download
from langchain.llms import LlamaCpp
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
import os
from langchain.llms import CTransformers
from sentence_transformers import SentenceTransformer
from langchain.chains.question_answering import load_qa_chain
# LangChain components to use
from langchain.vectorstores.cassandra import Cassandra
from langchain.indexes.vectorstore import VectorStoreIndexWrapper

# Support for dataset retrieval with Hugging Face
from datasets import load_dataset

# With CassIO, the engine powering the Astra DB integration in LangChain,
# you will also initialize the DB connection:
import cassio

# Callbacks support token-wise streaming
callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])
# Verbose is required to pass to the callback manager

os.environ["HUGGINGFACEHUB_API_TOKEN"] = "Hugging Face API"
ASTRA_DB_APPLICATION_TOKEN = os.environ.get('ASTRA_DB_APPLICATION_TOKEN', 'AstraDB API')
ASTRA_DB_ID = os.environ.get('ASTRA_DB_ID', 'AstraDB DatabaseID')

cassio.init(token=ASTRA_DB_APPLICATION_TOKEN, database_id=ASTRA_DB_ID)

embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')

astra_vector_store = Cassandra(
    embedding=embeddings,
    table_name="qa_mini_demo",
    session=None,
    keyspace=None,
)

# astra_vector_store.add_texts(texts)

# print("Inserted %i headlines." % len(texts))

astra_vector_index = VectorStoreIndexWrapper(vectorstore=astra_vector_store)

model_name_or_path = "TheBloke/CodeLlama-13B-Python-GGUF"
model_basename = "codellama-13b-python.Q5_K_M.gguf"
model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)

llm = CTransformers(
        model = "TheBloke/Llama-2-7B-Chat-GGML",
        model_type="llama",
        max_new_tokens = 800,
        temperature = 0.5
    )

query_text ="What happened at the Al-Shifa Hospital?"

astra_vector_store.similarity_search_with_score(query_text, k=4)

first_question = True
while True:
    if first_question:
        query_text = input("\nEnter your question (or type 'quit' to exit): ").strip()
    else:
        query_text = input("\nWhat's your next question (or type 'quit' to exit): ").strip()

    if query_text.lower() == "quit":
        break

    if query_text == "":
        continue

    first_question = False

    print("\nQUESTION: \"%s\"" % query_text)
    answer = astra_vector_index.query(query_text, llm=llm).strip()
    print("ANSWER: \"%s\"\n" % answer)

    print("FIRST DOCUMENTS BY RELEVANCE:")
    for doc, score in astra_vector_store.similarity_search_with_score(query_text, k=4):
        print("    [%0.4f] \"%s ...\"" % (score, doc.page_content[:84]))

"""#With llamaIndex"""

!pip install -q transformers einops accelerate langchain bitsandbytes

!pip install llama_index

# Commented out IPython magic to ensure Python compatibility.
# %pip install llama-index-llms-huggingface

from llama_index.core import VectorStoreIndex,ServiceContext,PromptTemplate,SimpleDirectoryReader
from llama_index.llms.huggingface import HuggingFaceLLM

system_prompt="""
You are a Q&A assistant. Your goal is to answer questions as
accurately as possible based on the instructions and context provided.
"""
## Default format supportable by LLama2
query_wrapper_prompt=PromptTemplate("<|USER|>{query_str}<|ASSISTANT|>")

!huggingface-cli login

import accelerate
import torch

llm = HuggingFaceLLM(
    context_window=4096,
    max_new_tokens=256,
    generate_kwargs={"temperature": 0.0, "do_sample": False},
    system_prompt=system_prompt,
    query_wrapper_prompt=query_wrapper_prompt,
    tokenizer_name="syedzaidi-kiwi/Llama-2-7b-chat-finetune",
    model_name="syedzaidi-kiwi/Llama-2-7b-chat-finetune",
    device_map="auto",
    # uncomment this if using CUDA to reduce memory usage
    model_kwargs={"torch_dtype": torch.float16 , "load_in_8bit":True}
)

## Embedding
!pip install install sentence_transformers

!pip install langchain-community

from langchain.embeddings.huggingface import HuggingFaceEmbeddings
from llama_index.core.indices.service_context import ServiceContext
from llama_index.legacy.embeddings.langchain import LangchainEmbedding


embed_model=LangchainEmbedding(
    HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2"))

service_context=ServiceContext.from_defaults(
    chunk_size=1024,
    llm=llm,
    embed_model=embed_model
)

service_context

documents = SimpleDirectoryReader('/content/Data').load_data()

documents

index=VectorStoreIndex.from_documents(documents,service_context=service_context)

query_engine=index.as_query_engine()

from llama_index.retrievers import VectorIndexRetriever
from llama_index.query_engine import RetrieverQueryEngine
from llama_index.indices.postprocessor import SimilarityPostprocessor

retriever=VectorIndexRetriever(index=index,similarity_top_k=4)
postprocessor=SimilarityPostprocessor(similarity_cutoff=0.80)

query_engine=RetrieverQueryEngine(retriever=retriever,
                                  node_postprocessors=[postprocessor])

first_question = True
while True:
    if first_question:
        query_text = input("\nEnter your question (or type 'quit' to exit): ").strip()
    else:
        query_text = input("\nWhat's your next question (or type 'quit' to exit): ").strip()

    if query_text.lower() == "quit":
        break

    if query_text == "":
        continue

    first_question = False

    print("\nQUESTION: \"%s\"" % query_text)
    response=query_engine.query(query_text)
    print("ANSWER: \"%s\"\n" % response)

    print("FIRST DOCUMENTS BY RELEVANCE:")
    for doc, score in astra_vector_store.similarity_search_with_score(query_text, k=4):
        print("    [%0.4f] \"%s ...\"" % (score, doc.page_content[:84]))
